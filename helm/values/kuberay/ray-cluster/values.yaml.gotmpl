image:
  repository: rayproject/ray
  {{ if (eq .Values.clusterType "minikube") }}
  tag: 2.34.0-py310-aarch64
  {{ else }}
  tag: 2.34.0-py310
  {{ end }}
  pullPolicy: IfNotPresent
service:
  type: ClusterIP

nameOverride: "kuberay"
fullnameOverride: ""

head:
  rayStartParams:
    num-cpus: '0' # to disable task scheduling on head node
    port: '6379'
    dashboard-host: '0.0.0.0'
    block: 'true'
  # rayVersion determines the autoscaler's image version.
#  rayVersion: 2.22.0
#  enableInTreeAutoscaling: true
#   TODO set autoscaler resources
#  autoscalerOptions:
#    upscalingMode: Default
#    idleTimeoutSeconds: 60
  containerEnv:
    - name: MY_POD_IP
      valueFrom:
        fieldRef:
          fieldPath: status.podIP
  resources:
    {{ if (eq .Values.clusterType "minikube") }}
    limits:
      memory: 3Gi
    requests:
      memory: 3Gi
    {{ else }}
    limits:
      memory: 4Gi
    requests:
      memory: 3Gi
    {{ end }}
  annotations: {}
  volumes:
    - name: log-volume
      emptyDir: {}
  volumeMounts:
    - mountPath: /tmp/ray
      name: log-volume
  sidecarContainers: []
  {{ if not (eq .Values.clusterType "minikube") }}
  tolerations:
    - key: workload-type
      operator: Equal
      value: ray
      effect: NoSchedule
    - key: ray-role
      operator: Equal
      value: head
      effect: NoSchedule
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
          - matchExpressions:
              - key: workload-type
                operator: In
                values:
                  - ray
              - key: ray-role
                operator: In
                values:
                  - head

  command:
    - conda install -y -c conda-forge gcc=12.1.0
  {{ end }}

worker:
  {{ if (eq .Values.clusterType "minikube") }}
  replicas: 9
  {{ else }}
  replicas: 140
  {{ end }}
  minReplicas: 0
  maxReplicas: 1000

  # If you want to disable the default workergroup
  # uncomment the line below
  # disabled: true
  groupName: test-group
  rayStartParams:
#    {{ if (eq .Values.clusterType "minikube") }}
#    num-cpus: '4'
#    {{ else }}
#    num-cpus: '8'
#    {{ end }}
#    resources: '"{\"worker_size_small\": 9999999, \"instance_on_demand\": 9999999}"'
#    node-ip-address: $MY_POD_IP
    block: 'true'
  containerEnv:
    - name: MY_POD_IP
      valueFrom:
        fieldRef:
          fieldPath: status.podIP
    - name: RAY_DISABLE_DOCKER_CPU_WARNING
      value: "1"
    - name: CPU_REQUEST
      valueFrom:
        resourceFieldRef:
          containerName: ray-worker
          resource: requests.cpu
    - name: MY_POD_NAME
      valueFrom:
        fieldRef:
          fieldPath: metadata.name
  {{ if (eq .Values.clusterType "minikube") }}
  resources:
    limits:
      memory: 8Gi
    requests:
      memory: 7Gi
  {{ else }}
  resources:
    limits:
      memory: 4Gi
    requests:
      memory: 3Gi
  {{ end }}
  volumes:
    - name: log-volume
      emptyDir: {}
  volumeMounts:
    - mountPath: /tmp/ray
      name: log-volume
  ports:
    - containerPort: 1234
      name: 'volga-tcp-0'
    - containerPort: 1235
      name: 'volga-tcp-1'
    - containerPort: 1236
      name: 'volga-tcp-2'
    - containerPort: 1237
      name: 'volga-tcp-3'
  {{ if not (eq .Values.clusterType "minikube") }}
  tolerations:
    - key: workload-type
      operator: Equal
      value: ray
      effect: NoSchedule
    - key: ray-role
      operator: Equal
      value: worker
      effect: NoSchedule
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
          - matchExpressions:
              - key: workload-type
                operator: In
                values:
                  - ray
              - key: ray-role
                operator: In
                values:
                  - worker
  command:
    - conda install -y -c conda-forge gcc=12.1.0
  {{ end }}